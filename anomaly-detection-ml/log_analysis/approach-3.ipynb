{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T20:52:45.342775Z",
     "iopub.status.busy": "2023-02-28T20:52:45.342389Z",
     "iopub.status.idle": "2023-02-28T20:52:45.351376Z",
     "shell.execute_reply": "2023-02-28T20:52:45.349894Z",
     "shell.execute_reply.started": "2023-02-28T20:52:45.342741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nJava/HDFS logs\\n\\nt1 INFO added user abc\\nt2 ERROR deleted user xyz\\n\\nElastic search\\n\\nt1 | INFO | added user abc\\nt2 | ERROR | deleted user xyz\\n\\nML \\n\\nEvents\\n\\nevent_id | event\\ne1 | added user <>\\ne2 | deleted user <>\\n\\n\\nLogs\\n\\n0-5 mins - tw1\\nt1 | INFO | added user 1\\nt2 | ERROR | deleted user 2\\nt3 | INFO | added user 3\\nt4 | ERROR | deleted user 4\\nt5 | INFO | added user 5\\n\\n\\n6-10 mins\\nt6 | ERROR | deleted user 6\\nt7 | INFO | added user 7\\nt8 | ERROR | deleted user 8\\n\\n11-15 mins\\nt9 | INFO | added user 9\\nt10 | ERROR | deleted user 0\\n\\n\\nLog_summary\\n\\n\\ntime_window_id | count_info | count_error | count_e1 | count_e2\\ntw1 | 3 | 2 | 3 | 2\\ntw2 | 1 | 2 | 1 | 2\\ntw3 | 1 | 1 | 1 | 1\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Java/HDFS logs\n",
    "\n",
    "t1 INFO added user abc\n",
    "t2 ERROR deleted user xyz\n",
    "\n",
    "Elastic search\n",
    "\n",
    "t1 | INFO | added user abc\n",
    "t2 | ERROR | deleted user xyz\n",
    "\n",
    "ML \n",
    "\n",
    "Events\n",
    "\n",
    "event_id | event\n",
    "e1 | added user <>\n",
    "e2 | deleted user <>\n",
    "\n",
    "\n",
    "Logs\n",
    "\n",
    "0-5 mins - tw1\n",
    "t1 | INFO | added user 1\n",
    "t2 | ERROR | deleted user 2\n",
    "t3 | INFO | added user 3\n",
    "t4 | ERROR | deleted user 4\n",
    "t5 | INFO | added user 5\n",
    "\n",
    "\n",
    "6-10 mins\n",
    "t6 | ERROR | deleted user 6\n",
    "t7 | INFO | added user 7\n",
    "t8 | ERROR | deleted user 8\n",
    "\n",
    "11-15 mins\n",
    "t9 | INFO | added user 9\n",
    "t10 | ERROR | deleted user 0\n",
    "\n",
    "\n",
    "Log_summary\n",
    "\n",
    "\n",
    "time_window_id | count_info | count_error | count_e1 | count_e2\n",
    "tw1 | 3 | 2 | 3 | 2\n",
    "tw2 | 1 | 2 | 1 | 2\n",
    "tw3 | 1 | 1 | 1 | 1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-13 23:13:38.229526 2023-04-14 23:13:38.229526\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "    \n",
    "end_time = datetime.datetime.utcnow()\n",
    "start_time = end_time - datetime.timedelta(days = 1)\n",
    "\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/7h0bt82n7g916rsm87krcvc00000gn/T/ipykernel_70148/2086235941.py:8: DeprecationWarning: The 'timeout' parameter is deprecated in favor of 'request_timeout'\n",
      "  es = Elasticsearch([\"{}:{}\".format(AWS_HOSTNAME, ELASTIC_PORT)],timeout=30)\n"
     ]
    }
   ],
   "source": [
    "# Elasticsearch connection\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "AWS_HOSTNAME = 'http://ec2-18-144-169-193.us-west-1.compute.amazonaws.com'\n",
    "ELASTIC_PORT = 9200\n",
    "\n",
    "es = Elasticsearch([\"{}:{}\".format(AWS_HOSTNAME, ELASTIC_PORT)],timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs fetched from elasticsearch and file is generated  at data/unstructured/Java/application.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vc/7h0bt82n7g916rsm87krcvc00000gn/T/ipykernel_70148/2215065275.py:14: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  search_result = es.search(index='spring-elk-logs', body=search_body) # replace log_index with your index name\n",
      "/var/folders/vc/7h0bt82n7g916rsm87krcvc00000gn/T/ipykernel_70148/2215065275.py:14: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  search_result = es.search(index='spring-elk-logs', body=search_body) # replace log_index with your index name\n"
     ]
    }
   ],
   "source": [
    "def fetch_logs(start_time, end_time):\n",
    "\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                    \"gte\": start_time,\n",
    "                    \"lte\": end_time\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    search_result = es.search(index='spring-elk-logs', body=search_body)\n",
    "    hits = search_result['hits']['hits']\n",
    "    log_dir = \"data/unstructured/Java/\"\n",
    "    log_file_path = os.path.join(log_dir, f\"application.log\")\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        for hit in hits:\n",
    "            log_file.write(f\"{hit['_source']['message']}\\n\")\n",
    "    return log_file_path\n",
    "\n",
    "log_file_path = fetch_logs(start_time, end_time)\n",
    "print(f\"Logs fetched from elasticsearch and file is generated  at {log_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-28T20:52:45.354560Z",
     "iopub.status.busy": "2023-02-28T20:52:45.354121Z",
     "iopub.status.idle": "2023-02-28T20:52:45.380464Z",
     "shell.execute_reply": "2023-02-28T20:52:45.378281Z",
     "shell.execute_reply.started": "2023-02-28T20:52:45.354514Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from log_parser import Drain\n",
    "\n",
    "log_file_path = 'data/unstructured/Java/'\n",
    "#label_file_name = 'data/unstructured/HDFS/anomaly_label.csv'\n",
    "unstructured_log_filename = 'application.log'\n",
    "structured_log_file_path = 'data/structured/Java/'\n",
    "structured_log_filename = 'application.log_structured.csv'\n",
    "\n",
    "\n",
    "def parseLog(log_file_path, log_file_name, structured_log_file_path, log_type):\n",
    "    if log_type == 'HDFS':\n",
    "        log_format = '<Date> <Time> <Pid> <Level> <Component>: <Content>'\n",
    "        \n",
    "    if log_type == 'Java':\n",
    "        log_format = '<Date> <Time> <Pid> <Level> <Component> - <Content>'\n",
    "\n",
    "    # Regular expression list for optional preprocessing (default: [])\n",
    "    regex      = [\n",
    "        r'blk_(|-)[0-9]+' , # block id\n",
    "        r'(/|)([0-9]+\\.){3}[0-9]+(:[0-9]+|)(:|)', # IP\n",
    "        r'(?<=[^A-Za-z0-9])(\\-?\\+?\\d+)(?=[^A-Za-z0-9])|[0-9]+$', # Numbers\n",
    "    ]\n",
    "    st         = 0.5  # Similarity threshold\n",
    "    depth      = 4  # Depth of all leaf nodes\n",
    "\n",
    "    parser = Drain.LogParser(log_format, indir=log_file_path, outdir=structured_log_file_path,  depth=depth, st=st, rex=regex)\n",
    "    parser.parse(log_file_name)\n",
    "\n",
    "## parse the logs - convert unstructured to structured log\n",
    "parseLog(log_file_path, unstructured_log_filename, structured_log_file_path, 'Java')\n",
    "    \n",
    "\n",
    "## read structured log \n",
    "print(\"Loading\", structured_log_file_path+structured_log_filename)\n",
    "structured_log = pd.read_csv(structured_log_file_path+structured_log_filename, engine='c', na_filter=False, memory_map=True)\n",
    "\n",
    "structured_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter null dates\n",
    "\n",
    "structured_log = structured_log[structured_log['Date'] != '']\n",
    "structured_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T20:52:45.381949Z",
     "iopub.status.idle": "2023-02-28T20:52:45.382918Z",
     "shell.execute_reply": "2023-02-28T20:52:45.382628Z",
     "shell.execute_reply.started": "2023-02-28T20:52:45.382597Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_zeros(x):\n",
    "    if len(str(x)) < 6:\n",
    "        return str(x).zfill(6)\n",
    "    else:\n",
    "        return str(x)\n",
    "    \n",
    "structured_log['Date'] = structured_log['Date'].apply(fill_zeros)\n",
    "structured_log['Time'] = structured_log['Time'].apply(fill_zeros)\n",
    "\n",
    "structured_log.loc[:,'Date'] = pd.to_datetime(structured_log.Date.astype(str)+' '+structured_log.Time.astype(str), format=\"%d-%m-%Y %H:%M:%S.%f\")\n",
    "# structured_log.set_index(\"Date\", inplace=True)\n",
    "\n",
    "structured_log = structured_log.drop(columns=['Time'])\n",
    "\n",
    "structured_log.head()\n",
    "\n",
    "# structured_log['EventTemplate'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T20:52:45.384456Z",
     "iopub.status.idle": "2023-02-28T20:52:45.385636Z",
     "shell.execute_reply": "2023-02-28T20:52:45.385400Z",
     "shell.execute_reply.started": "2023-02-28T20:52:45.385371Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Cleaning: Remove rows and colums with count 0 (no data present)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T20:52:45.387758Z",
     "iopub.status.idle": "2023-02-28T20:52:45.388406Z",
     "shell.execute_reply": "2023-02-28T20:52:45.388142Z",
     "shell.execute_reply.started": "2023-02-28T20:52:45.388109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding relevant columns to the dataframe\n",
    "LOG_LEVELS = ['WARN', 'INFO', 'DEBUG', 'TRACE', 'ERROR', 'FATAL']\n",
    "df_grouped = structured_log.groupby(pd.Grouper(key='Date', freq='5Min',closed='right',label='right')).agg(\n",
    "    total_msgs=pd.NamedAgg(column=\"Content\", aggfunc=\"count\"),    \n",
    ").reset_index()\n",
    "\n",
    "for level in LOG_LEVELS:\n",
    "    df_grouped[level + '_count'] = 0\n",
    "\n",
    "for event_id in structured_log['EventId'].unique():\n",
    "    df_grouped[event_id + '_count'] = 0\n",
    "\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T20:52:45.390342Z",
     "iopub.status.idle": "2023-02-28T20:52:45.390933Z",
     "shell.execute_reply": "2023-02-28T20:52:45.390712Z",
     "shell.execute_reply.started": "2023-02-28T20:52:45.390686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Populating all the log level counts\n",
    "df_grouped_logLevel = structured_log.groupby([pd.Grouper(key='Date', freq='5Min',closed='right',label='right'), 'Level']).agg(\n",
    "    count=pd.NamedAgg(column=\"Level\", aggfunc=\"count\"),    \n",
    ").reset_index()\n",
    "\n",
    "for row in df_grouped_logLevel.itertuples():\n",
    "    df_grouped.loc[df_grouped['Date'] == row.Date, row.Level + '_count'] = row.count\n",
    "\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-28T20:52:45.392384Z",
     "iopub.status.idle": "2023-02-28T20:52:45.392828Z",
     "shell.execute_reply": "2023-02-28T20:52:45.392631Z",
     "shell.execute_reply.started": "2023-02-28T20:52:45.392604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Populating all the event id counts\n",
    "df_grouped_eventId = structured_log.groupby([pd.Grouper(key='Date', freq='5Min',closed='right',label='right'), 'EventId']).agg(\n",
    "    count=pd.NamedAgg(column=\"EventId\", aggfunc=\"count\"),    \n",
    ").reset_index()\n",
    "\n",
    "for row in df_grouped_eventId.itertuples():\n",
    "    df_grouped.loc[df_grouped['Date'] == row.Date, row.EventId + '_count'] = row.count\n",
    "\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually labelling anomalous window to compare later with model output\n",
    "\n",
    "df_grouped['anomaly_manual'] = (df_grouped['ERROR_count'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only the feature columns\n",
    "\n",
    "feature_cols = list(df_grouped.columns);\n",
    "feature_cols.remove('Date')\n",
    "\n",
    "data_with_feature_columns = df_grouped[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the values of the input data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "data_scaled = min_max_scaler.fit_transform(data_with_feature_columns)\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=feature_cols)\n",
    "\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the relationship between log_levels and the event_ids\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "all_columns = list(df_grouped.columns)\n",
    "log_level_columns = all_columns[2:8]\n",
    "event_id_columns = all_columns[8:]\n",
    "\n",
    "sns.pairplot(df_grouped, y_vars= event_id_columns,\n",
    "                  x_vars= log_level_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Kmeans clustering to different values of k (1-15)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "RANDOM_STATE = 123\n",
    "\n",
    "cluster_check_range = range(1, 15)\n",
    "\n",
    "kmeans = [None] * (len(cluster_check_range) + 1)\n",
    "scores = [0] * (len(cluster_check_range) + 1)\n",
    "for i in cluster_check_range:\n",
    "    if i == 0:\n",
    "        continue\n",
    "    kmeans[i] = KMeans(n_clusters=i, random_state=RANDOM_STATE).fit(data_scaled) \n",
    "    scores[i] = kmeans[i].score(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the elbow point\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cluster_check_range, scores[1:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting appropriate k. Here we chose k = 8\n",
    "\n",
    "k = 2\n",
    "cluster_model = kmeans[k]\n",
    "\n",
    "df_grouped['cluster'] = cluster_model.predict(data_scaled)\n",
    "df_grouped['cluster'].value_counts()\n",
    "df = df_grouped['cluster'].value_counts().rename_axis('Cluster number').reset_index(name='counts')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data - df_grouped\n",
    "# data_new - data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying tSNE to visualise data in 2D\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300, random_state=RANDOM_STATE)\n",
    "tsne_results = tsne.fit_transform(data_scaled)\n",
    "\n",
    "df_grouped['tsne-x-axis'] = tsne_results[:,0]\n",
    "df_grouped['tsne-y-axis'] = tsne_results[:,1]\n",
    "df_grouped\n",
    "\n",
    "tsne_cluster = df_grouped.groupby('cluster').agg({'tsne-x-axis':'mean', 'tsne-y-axis':'mean'}).reset_index()\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-x-axis\", y=\"tsne-y-axis\",\n",
    "    hue=\"cluster\",\n",
    "    palette=sns.color_palette(\"hls\", k),\n",
    "    data=df_grouped,\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")\n",
    "\n",
    "plt.scatter(x=\"tsne-x-axis\", y=\"tsne-y-axis\", data=tsne_cluster, s=100, c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histogram of sum_squared_distances of all points from the center of clusters\n",
    "\n",
    "def get_ssd(data, cluster_model, feature_cols):\n",
    "    centers = cluster_model.cluster_centers_\n",
    "    points = np.asarray(data[feature_cols])\n",
    "    total_distance = pd.Series()\n",
    "    for i in range(len(points)):\n",
    "        total_distance.at[i] = get_distance(centers, points, i)\n",
    "    return total_distance  \n",
    "\n",
    "def get_distance(centers, points, i):\n",
    "    distance = 0\n",
    "    for j in range(len(centers)):\n",
    "        d = np.linalg.norm(points[i] - centers[j])\n",
    "        distance += d**2\n",
    "    return distance\n",
    "\n",
    "\n",
    "centers = cluster_model.cluster_centers_\n",
    "points = np.asarray(data_scaled)\n",
    "\n",
    "df_grouped['ssd'] = get_ssd(data_scaled, cluster_model, feature_cols)\n",
    "\n",
    "plt.hist(df_grouped['ssd'], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting cutoff to ssd for anomaly\n",
    "\n",
    "cutoff = 8.5\n",
    "df_grouped['anomaly_kmeans'] = (df_grouped['ssd'] >= cutoff).astype(int)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-x-axis\", y=\"tsne-y-axis\",\n",
    "    hue=\"anomaly_kmeans\",\n",
    "    data=df_grouped,\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing anomalous rows according to k-means\n",
    "\n",
    "df_grouped.loc[df_grouped['anomaly_kmeans']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering using Isolated forests\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "outlier_fraction = 0.03\n",
    "\n",
    "model =  IsolationForest(n_jobs=-1, n_estimators=200, max_features=3, random_state=RANDOM_STATE, contamination=outlier_fraction)\n",
    "model.fit(data_scaled)\n",
    "\n",
    "df_grouped['anomaly_isolated'] = pd.Series(model.predict(data_scaled))\n",
    "df_grouped['anomaly_isolated'] = df_grouped['anomaly_isolated'].map( {1: 0, -1: 1} )\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-x-axis\", y=\"tsne-y-axis\",\n",
    "    hue=\"anomaly_isolated\",\n",
    "    data=df_grouped,\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")\n",
    "\n",
    "df_grouped.loc[df_grouped['anomaly_isolated']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    sensitivity = cf[0,0]/(cf[:,0].sum())\n",
    "    specificity = cf[1,1]/(cf[:,1].sum())\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return {'f1_score': f1, 'accuracy': acc, 'sensitivity': sensitivity, 'specificity': specificity}\n",
    "\n",
    "anomaly_manual = df_grouped['anomaly_manual']\n",
    "anomaly_isolated = df_grouped['anomaly_isolated']\n",
    "anomaly_kmeans = df_grouped['anomaly_kmeans']\n",
    "\n",
    "kmeans_metrics = calculate_metrics(anomaly_manual, anomaly_isolated)\n",
    "iso_metrics = calculate_metrics(anomaly_manual, anomaly_kmeans)\n",
    "\n",
    "\n",
    "# Create dataframes from the metrics dictionaries\n",
    "kmeans_df = pd.DataFrame.from_dict(kmeans_metrics, orient='index', columns=['K-means'])\n",
    "iso_df = pd.DataFrame.from_dict(iso_metrics, orient='index', columns=['Isolation Forest'])\n",
    "\n",
    "# Combine the dataframes\n",
    "metrics_df = pd.concat([kmeans_df, iso_df], axis=1)\n",
    "\n",
    "# Print the dataframe\n",
    "print(metrics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_comparison(kmeans_metrics, iso_metrics):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(kmeans_metrics.keys(), kmeans_metrics.values(), width=-0.4, align='edge', label='K-means')\n",
    "    ax.bar(iso_metrics.keys(), iso_metrics.values(), width=0.4, align='edge', label='Isolation Forest')\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Model Comparison')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_metric_comparison(kmeans_metrics, iso_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(models, true_labels):\n",
    "    n_models = len(models)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 5), sharey='row')\n",
    "    \n",
    "    for i, (model_name, y_pred) in enumerate(models.items()):\n",
    "        cm = confusion_matrix(true_labels, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
    "        disp.plot(ax=axes[i])\n",
    "        disp.im_.colorbar.remove()\n",
    "        disp.ax_.set_title(f\"Confusion Matrix for {model_name}\")\n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.6, hspace=0.01)\n",
    "   \n",
    "    plt.show()\n",
    "    \n",
    "models = {\n",
    "    'K Means': df_grouped['anomaly_kmeans'],\n",
    "    'Isolated Forest': df_grouped['anomaly_isolated']\n",
    "}\n",
    "\n",
    "true_labels = df_grouped['anomaly_manual']\n",
    "plot_confusion_matrices(models, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
